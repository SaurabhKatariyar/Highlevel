{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2dbcc7-51ea-405c-be4a-d16119ca77f3",
   "metadata": {},
   "source": [
    "### Check spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab30e119-304d-498a-996b-4ad8a9264c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 17:57:47 WARN Utils: Your hostname, saurabhk-ThinkPad-E14-Gen-3 resolves to a loopback address: 127.0.1.1; using 192.168.1.14 instead (on interface wlp3s0)\n",
      "24/08/09 17:57:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/09 17:57:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'getExecutorMemoryStatus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Get details about the executors\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m executors \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetExecutorMemoryStatus\u001b[49m()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecutor Memory Status:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m executor, (total_memory, free_memory) \u001b[38;5;129;01min\u001b[39;00m executors\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'getExecutorMemoryStatus'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkStatus\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Get details about the executors\n",
    "executors = sc.getExecutorMemoryStatus()\n",
    "print(\"Executor Memory Status:\")\n",
    "for executor, (total_memory, free_memory) in executors.items():\n",
    "    print(f\"Executor ID: {executor}\")\n",
    "    print(f\"  Total Memory: {total_memory / (1024**2):.2f} MB\")\n",
    "    print(f\"  Free Memory: {free_memory / (1024**2):.2f} MB\")\n",
    "    print('-' * 40)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856d3413-a2a8-45dc-bf51-013c52e711e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 20:34:34 WARN Utils: Your hostname, saurabhk-ThinkPad-E14-Gen-3 resolves to a loopback address: 127.0.1.1; using 192.168.1.14 instead (on interface wlp3s0)\n",
      "24/08/09 20:34:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/09 20:34:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Memory Information:\n",
      "Total Memory: 14.45 GB\n",
      "Available Memory: 4.36 GB\n",
      "Used Memory: 9.44 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkStatus\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Log system memory details\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(\"System Memory Information:\")\n",
    "print(f\"Total Memory: {memory_info.total / (1024**3):.2f} GB\")\n",
    "print(f\"Available Memory: {memory_info.available / (1024**3):.2f} GB\")\n",
    "print(f\"Used Memory: {memory_info.used / (1024**3):.2f} GB\")\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff11689-4957-4386-8bcb-a5ed40667a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+\n",
      "| Name|Age|Age Next Year|\n",
      "+-----+---+-------------+\n",
      "|Alice| 34|           35|\n",
      "|  Bob| 45|           46|\n",
      "|Cathy| 29|           30|\n",
      "|David| 35|           36|\n",
      "+-----+---+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SimplePySparkJob\").master(\"spark://192.168.1.14:7077\").getOrCreate()\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29), (\"David\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Perform a simple transformation\n",
    "df_transformed = df.withColumn(\"Age Next Year\", df[\"Age\"] + 1)\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "df_transformed.show()\n",
    "\n",
    "# Perform an action to trigger execution\n",
    "count = df_transformed.count()\n",
    "print(f\"Total records: {count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec7623-b529-4019-a427-313e9880fccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
